{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanushree-Kadgi/Deepfake-Detection-System/blob/main/deepfake_vs_real_face_detection_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TVG4fKYdGkdB",
      "metadata": {
        "id": "TVG4fKYdGkdB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "a123f86c-1fbd-4409-be3e-ac68d396c670"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-83cd0558-ece4-4ca3-9068-47cc16739864\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-83cd0558-ece4-4ca3-9068-47cc16739864\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (2).json\n"
          ]
        }
      ],
      "source": [
        "# Create Kaggle API to Upload the Dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hwg5BV94HmWz",
      "metadata": {
        "id": "Hwg5BV94HmWz"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u2W_k8RVHpGH",
      "metadata": {
        "id": "u2W_k8RVHpGH"
      },
      "outputs": [],
      "source": [
        "#Creates a hidden folder named .kaggle in home directory.\n",
        "#Kaggle API requires a folder ~/.kaggle to store your API key.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0v5yC0ifFqNo",
      "metadata": {
        "id": "0v5yC0ifFqNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50267260-7e70-4868-dfd9-2d1f640478ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "! pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9NFrzF_-FyG3",
      "metadata": {
        "id": "9NFrzF_-FyG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21d2000a-c48e-4827-818e-5d0449109ffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " deepfake-and-real-images.zip  'kaggle (1).json'   \u001b[0m\u001b[01;34msample_data\u001b[0m/\n",
            " \u001b[01;34mdeepfake_dataset\u001b[0m/             'kaggle (2).json'\n",
            " \u001b[01;34mdrive\u001b[0m/                         kaggle.json\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2_waJgLuHr7V",
      "metadata": {
        "id": "2_waJgLuHr7V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd39a8b-2221-4538-9a66-e013dd2cfeea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/manjilkarki/deepfake-and-real-images\n",
            "License(s): unknown\n",
            "deepfake-and-real-images.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d manjilkarki/deepfake-and-real-images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pKehwqvhHt-5",
      "metadata": {
        "id": "pKehwqvhHt-5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2930a91-432c-4428-9af3-b65b29e42c89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  deepfake-and-real-images.zip\n",
            "replace /content/deepfake_dataset/Dataset/Test/Fake/fake_0.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip deepfake-and-real-images.zip -d /content/deepfake_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0675bf8-7fed-4d05-bdff-36d11b96654c",
      "metadata": {
        "id": "c0675bf8-7fed-4d05-bdff-36d11b96654c"
      },
      "outputs": [],
      "source": [
        "#Importing necessary libraries and modules\n",
        "import warnings #for handling warnings\n",
        "warnings.filterwarnings(\"ignore\") #Ignore warnings during execution\n",
        "\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    f1_score\n",
        ")\n",
        "\n",
        "#Import custom modules and classes\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import accelerate\n",
        "!pip install evaluate # Install the missing 'evaluate' library\n",
        "import evaluate\n",
        "from datasets import Dataset, Image, ClassLabel\n",
        "from transformers import ( #Import various modules from the transformer library\n",
        "    TrainingArguments,#For training arguments\n",
        "    Trainer, #For model Training\n",
        "    ViTImageProcessor, #For processing image data with ViT models\n",
        "    ViTForImageClassification, #ViT model foe image classification\n",
        "    DefaultDataCollator #For collating data in the default way\n",
        ")\n",
        "\n",
        "import torch #Import PyTorch for Deep Learning\n",
        "from torch.utils.data import DataLoader #For creating data loaders\n",
        "from torchvision.transforms import ( #Import image transformer functions\n",
        "    CenterCrop, #Center crop image\n",
        "    Compose, #Compose multiple image transformation\n",
        "    Normalize, #Normalize image pixel values\n",
        "    RandomRotation, #Apply random rotation to images\n",
        "    RandomResizedCrop, #Crop and resize images randomly\n",
        "    RandomHorizontalFlip, #Apply random horizontal flip\n",
        "    RandomAdjustSharpness, #Adjust sharpness randomly\n",
        "    Resize, #Resize images\n",
        "    ToTensor #Convert images to Pytorch tensors\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ecd710b-a352-4e10-a25e-f20c40a899dd",
      "metadata": {
        "id": "8ecd710b-a352-4e10-a25e-f20c40a899dd"
      },
      "outputs": [],
      "source": [
        "#Import the necessary module from the Python Imaging Library (PIL)\n",
        "from PIL import ImageFile\n",
        "\n",
        "#Enable the option to load truncated images\n",
        "#This setting allows the PIL library to attempt loding images even if they are corrupted or incomplete.\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba1e3f54-1dda-41e0-bc95-caa56b10faa7",
      "metadata": {
        "id": "ba1e3f54-1dda-41e0-bc95-caa56b10faa7"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "image_dict ={}\n",
        "\n",
        "# Define list of file names\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "#Initialize empty lists to store file names and labels\n",
        "file_names = []\n",
        "labels = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92faaecd-cf41-4399-8b75-d8c18c8c5373",
      "metadata": {
        "id": "92faaecd-cf41-4399-8b75-d8c18c8c5373"
      },
      "outputs": [],
      "source": [
        "\n",
        "#for file in sorted((Path('D:/DEEPFAKE Detection System/Dataset/Test').glob('**/*.*'))):\n",
        "    #label = str(file).split('/')[-2]\n",
        "\n",
        "# IMPORTANT: Update this path to your actual dataset location in Google Drive\n",
        "data_path = Path(r\"deepfake_dataset/Dataset/Test\") # Example path, adjust as needed\n",
        "file_names = []\n",
        "labels = []\n",
        "for file in sorted(data_path.glob(\"**/*.*\")):\n",
        "    if not file.is_file():\n",
        "        continue\n",
        "    # rel is path relative to the Test folder, e.g. \"Fake/image1.jpg\" -> parts = ('Fake', 'image1.jpg')\n",
        "    try:\n",
        "        rel = file.relative_to(data_path)\n",
        "    except Exception as e:\n",
        "        # fallback: use parent name\n",
        "        rel = file\n",
        "    parts = rel.parts\n",
        "    if len(parts) >= 1:\n",
        "        label = parts[0]               # first folder under Test -> Fake or Real\n",
        "    else:\n",
        "        label = file.parent.name       # fallback\n",
        "\n",
        "    file_names.append(str(file))\n",
        "    labels.append(label)\n",
        "\n",
        "print(\"Total files:\", len(file_names), \"Total labels:\", len(labels))\n",
        "\n",
        "df = pd.DataFrame({\"image\": file_names, \"label\": labels})\n",
        "print(df.shape)\n",
        "print(df[\"label\"].value_counts())   # quick sanity check of labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a06d6ead-2b19-4464-8378-db8319bb4d89",
      "metadata": {
        "id": "a06d6ead-2b19-4464-8378-db8319bb4d89"
      },
      "outputs": [],
      "source": [
        "# show first 10 rows for verification\n",
        "df.head(100000)\n",
        "df[['image', 'label']].head(100000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c367abe-d699-4c19-a2c1-80d76830acf2",
      "metadata": {
        "id": "3c367abe-d699-4c19-a2c1-80d76830acf2"
      },
      "outputs": [],
      "source": [
        "df['label'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "btKmbi3X5xmt",
      "metadata": {
        "id": "btKmbi3X5xmt"
      },
      "outputs": [],
      "source": [
        "df.shape, df.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cff12cf-9e65-4be7-b74c-0d82e8b31f32",
      "metadata": {
        "id": "3cff12cf-9e65-4be7-b74c-0d82e8b31f32"
      },
      "outputs": [],
      "source": [
        "\n",
        "# y contain the target variable (label) we want to predict\n",
        "y = df['label']\n",
        "\n",
        "# Drop the 'label' column from DataFrame to seperate features from target variables\n",
        "df = df.drop(['label'], axis=1)\n",
        "\n",
        "# Create a RandomOverSampler object with a specified random seed (random_state=83)\n",
        "ros = RandomOverSampler(random_state=83)\n",
        "\n",
        "#Use the RandomOverSampler to resample the dataset by oversampling the minority class\n",
        "df,y_resampled = ros.fit_resample(df, y)\n",
        "\n",
        "#Delete the original 'y' variable\n",
        "del y\n",
        "\n",
        "#Add resampled target variable\n",
        "df['label'] = y_resampled\n",
        "\n",
        "# Delete the y resampled variable as a new 'label' column is in DataFrame 'df'\n",
        "del y_resampled\n",
        "\n",
        "# Perform GC\n",
        "gc.collect()\n",
        "\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xBPVh0urflZJ",
      "metadata": {
        "id": "xBPVh0urflZJ"
      },
      "outputs": [],
      "source": [
        "# As UI is not created..This is only for TESTING purpose\n",
        "\n",
        "'''from google.colab import files\n",
        "from PIL import Image\n",
        "import io\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Move model to GPU\n",
        "model.to(\"cuda\")\n",
        "model.eval()\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename, data in uploaded.items():\n",
        "    img = Image.open(io.BytesIO(data)).convert(\"RGB\")\n",
        "    display(img)\n",
        "\n",
        "    # Preprocess\n",
        "    inputs = processor(images=img, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    probs = F.softmax(logits, dim=-1)        # convert to probabilities\n",
        "    preds = torch.argmax(probs, dim=-1).item()\n",
        "\n",
        "    # Confidence score\n",
        "    confidence = probs[0][preds].item()\n",
        "\n",
        "    # Label\n",
        "    label = model.config.id2label[preds]\n",
        "\n",
        "    print(\"---------------------------------\")\n",
        "    print(\"Prediction:\", label)\n",
        "    print(\"Confidence:\", round(confidence * 100, 2), \"%\")\n",
        "    print(\"---------------------------------\") '''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tEpzExddED5T",
      "metadata": {
        "id": "tEpzExddED5T"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19925081",
      "metadata": {
        "id": "19925081"
      },
      "outputs": [],
      "source": [
        "# After running the above cell and following the instructions, you can list the contents\n",
        "# of your Google Drive to verify it's mounted correctly.\n",
        "# For example, to list the contents of your root Google Drive folder:\n",
        "!ls /content/gdrive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbeded0f",
      "metadata": {
        "id": "dbeded0f"
      },
      "source": [
        "Once your Drive is mounted, you'll need to update the `data_path` in your code to point to the correct location of your dataset within your Google Drive. For example, if your 'Test' folder is directly inside 'MyDrive', the path would be `/content/gdrive/MyDrive/DEEPFAKE Detection System/Dataset/Test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7883fdce-8753-423a-81cc-9f2423f67789",
      "metadata": {
        "id": "7883fdce-8753-423a-81cc-9f2423f67789"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset.from_pandas(df).cast_column(\"image\", Image())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d937e354-fe51-497d-9029-47bf1065b0ea",
      "metadata": {
        "id": "d937e354-fe51-497d-9029-47bf1065b0ea"
      },
      "outputs": [],
      "source": [
        "dataset[0][\"image\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c61c220-3991-4711-84a3-8123e6da6b46",
      "metadata": {
        "id": "3c61c220-3991-4711-84a3-8123e6da6b46"
      },
      "outputs": [],
      "source": [
        "# Extracting subset of elements from the 'label' list using slicing\n",
        "labels_subset = labels[:5]\n",
        "\n",
        "# Printing subset of elements\n",
        "print(labels_subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd12ae05-97be-4861-803f-74610b4272cb",
      "metadata": {
        "id": "dd12ae05-97be-4861-803f-74610b4272cb"
      },
      "outputs": [],
      "source": [
        "#Creating list of unique lables by converting 'labels' to a set and then back to list\n",
        "labels_list = ['Real', 'Fake']\n",
        "\n",
        "#Initialize empty dictionaries to map labels to IDs and vice versa\n",
        "label2id, id2label = dict(), dict()\n",
        "\n",
        "#Iterate over the unique labels and assign each label an ID , and vice versa\n",
        "for i, label in enumerate(labels_list):\n",
        "    label2id[label] = i #Map label to its corresponding ID\n",
        "    id2label[i] = label #Map the ID to its corresponding label\n",
        "\n",
        "# Print the resulting dictionaries for references\n",
        "print(\"Mapping of IDs to Labels:\", id2label,'\\n')\n",
        "print(\"Mapping of Labels to IDs:\", label2id)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db320d29-3a60-4480-9b87-508bd95c06d7",
      "metadata": {
        "id": "db320d29-3a60-4480-9b87-508bd95c06d7"
      },
      "outputs": [],
      "source": [
        "# Creating classlabels to match labels to IDs\n",
        "ClassLabels = ClassLabel(num_classes=len(labels_list), names=labels_list)\n",
        "\n",
        "#Mapping labels to IDs\n",
        "def map_label2id(example):\n",
        "    example['label'] = ClassLabels.str2int(example['label'])\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(map_label2id, batched=True)\n",
        "\n",
        "#Casting label column to ClassLabel Object\n",
        "dataset = dataset.cast_column('label', ClassLabels)\n",
        "\n",
        "#Splliting the datasets into training and testing sets using 60-40 split ratio\n",
        "dataset = dataset.train_test_split(test_size=0.4, shuffle=True, stratify_by_column=\"label\")\n",
        "\n",
        "# Extracting the training data from the split dataset\n",
        "train_data = dataset['train']\n",
        "\n",
        "# Extracting the testing data from the split dataset\n",
        "test_data = dataset['test']\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f9311ec-5b09-4549-9bfa-f3255ed0fcd8",
      "metadata": {
        "id": "7f9311ec-5b09-4549-9bfa-f3255ed0fcd8"
      },
      "outputs": [],
      "source": [
        "# Define pre-trained ViT model string\n",
        "model_str = \"dima806/deepfake_vs_real_image_detection\"\n",
        "\n",
        "# Create processor for ViT model i/p from pretrained model\n",
        "processor = ViTImageProcessor.from_pretrained(model_str)\n",
        "\n",
        "# Rerive the image mean and standard deviation used for normalizarion\n",
        "image_mean, image_std = processor.image_mean, processor.image_std\n",
        "\n",
        "# Get size (height) of the ViT models i/p images\n",
        "size = processor.size[\"height\"]\n",
        "print(\"Size: \",size)\n",
        "\n",
        "# Define a normalization Transformation\n",
        "normalize = Normalize(mean=image_mean, std=image_std)\n",
        "\n",
        "#Define a Transformation for training data\n",
        "_train_transforms = Compose(\n",
        "    [\n",
        "        Resize((size, size)),\n",
        "        RandomRotation(90),\n",
        "        RandomAdjustSharpness(2),\n",
        "        ToTensor(),\n",
        "        normalize\n",
        "    ]\n",
        ")\n",
        "\n",
        "#Define a set of Transformation for validation data\n",
        "_val_transforms = Compose(\n",
        "    [\n",
        "        Resize((size,size)),\n",
        "        ToTensor(),\n",
        "        normalize\n",
        "    ]\n",
        ")\n",
        "\n",
        "#Define a function to apply training transformation to a batch examples\n",
        "def train_transforms(examples):\n",
        "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    return examples\n",
        "\n",
        "\n",
        "#Define a function to apply validation transformation to a batch examples\n",
        "def val_transforms(examples):\n",
        "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    return examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa6d9d5d-e729-49fc-893e-bf4dd5a1b24a",
      "metadata": {
        "id": "aa6d9d5d-e729-49fc-893e-bf4dd5a1b24a"
      },
      "outputs": [],
      "source": [
        "# Set transformers for training datasets\n",
        "train_data.set_transform(train_transforms)\n",
        "\n",
        "# Set transformers for test datasets\n",
        "test_data.set_transform(val_transforms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "381e77cd-e3a9-4ad8-b513-a2d3abd827a0",
      "metadata": {
        "id": "381e77cd-e3a9-4ad8-b513-a2d3abd827a0"
      },
      "outputs": [],
      "source": [
        "# Define a collate function that prepares batched data for model training\n",
        "def collate_fn(examples):\n",
        "    # Stack the pixel values from individual examples into a single tensor.\n",
        "    pixel_values = torch.stack([example [\"pixel_values\"] for example in examples])\n",
        "\n",
        "    # Convert the label strings in examples to corresponding numeric IDs using 1\n",
        "    labels = torch.tensor ([example ['label'] for example in examples])\n",
        "\n",
        "    # Return a dictionary containing the batched pixel values and labels.\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3338e44-51af-4a44-80f5-879981271c2e",
      "metadata": {
        "id": "e3338e44-51af-4a44-80f5-879981271c2e"
      },
      "outputs": [],
      "source": [
        "# Load , train and evaluate model\n",
        "\n",
        "#Create a VITForImageClassification model from a pretrained checkpoint with a specified number of output labels.\n",
        "model = ViTForImageClassification.from_pretrained (model_str, num_labels=len(labels_list))\n",
        "\n",
        "#Configure the mapping of class labels to their corresponding indices for later reference.\n",
        "model.config.id2label = id2label\n",
        "model.config.label2id = label2id\n",
        "\n",
        "#Calculate and print the number of trainable parameters in millions for the model.\n",
        "print(model.num_parameters(only_trainable=True) / 1e6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed266fa3-48da-473f-a5df-a49e2ffdaa0c",
      "metadata": {
        "id": "ed266fa3-48da-473f-a5df-a49e2ffdaa0c"
      },
      "outputs": [],
      "source": [
        "#Load accuracy metric\n",
        "#accuracy = evaluate.load(\"accuray\")\n",
        "from evaluate import load\n",
        "accuracy = load(\"glue\", \"mrpc\")\n",
        "\n",
        "#Define a function to calculate evaluation metric\n",
        "def compute_metrics(eval_pred):\n",
        "    # Extract model predictions from the evaluation prediction object\n",
        "    predictions = eval_pred.predictions\n",
        "\n",
        "    # Extract true labels from the evaluation prediction object\n",
        "    label_ids = eval_pred.label_ids\n",
        "\n",
        "    # Calculate accuracy using the loaded accuracy metric\n",
        "    # Convert model predictions to class labels by selecting the class with the highest probability (argmax)\n",
        "    predicted_labels = predictions.argmax(axis=1)\n",
        "\n",
        "    # Calculate accuracy score by comparing predicted labels to true labels\n",
        "    acc_score = accuracy.compute(predictions=predicted_labels, references=label_ids) ['accuracy']\n",
        "\n",
        "    # Return the computed accuracy as a dictionary with the key \"accuracy\"\n",
        "    return {\n",
        "        \"accuracy\": acc_score\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0bc03be-17b5-4afb-9337-8e3334047f3d",
      "metadata": {
        "id": "e0bc03be-17b5-4afb-9337-8e3334047f3d"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48146c30-9b78-4eb1-86b4-2fd61f74f4ce",
      "metadata": {
        "id": "48146c30-9b78-4eb1-86b4-2fd61f74f4ce"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "metric_name = \"accuracy\"\n",
        "model_name =\"deepfake_vs_real_image_detection\"\n",
        "num_train_epochs = 2\n",
        "\n",
        "args = TrainingArguments(\n",
        "\n",
        "    output_dir=model_name,\n",
        "    logging_dir='./logs',\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=1e-6,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    weight_decay=0.02,\n",
        "    warmup_steps=50,\n",
        "    remove_unused_columns=False,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=1,\n",
        "    report_to=\"none\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bde0efa-4582-4884-8100-7fa9c646f382",
      "metadata": {
        "id": "0bde0efa-4582-4884-8100-7fa9c646f382"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "print(\"TrainingArguments is from:\", TrainingArguments.__module__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfa7d8ad-d5e0-4068-a3f7-1cd5b2474979",
      "metadata": {
        "id": "bfa7d8ad-d5e0-4068-a3f7-1cd5b2474979"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "# Create a Trainer instaance for fine-tuning a language model\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=test_data,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13c0e0ab-bdb3-4739-b5c3-2452b049e4e1",
      "metadata": {
        "id": "13c0e0ab-bdb3-4739-b5c3-2452b049e4e1"
      },
      "outputs": [],
      "source": [
        "# Start training the model using the trainer object\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbRU-dZs_1w8",
      "metadata": {
        "id": "bbRU-dZs_1w8"
      },
      "outputs": [],
      "source": [
        "# Evaluate the post training model\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KtE1o31GAogG",
      "metadata": {
        "id": "KtE1o31GAogG"
      },
      "outputs": [],
      "source": [
        "# Use the trained 'trainer' to make predictions\n",
        "outputs = trainer.predict(test_data)\n",
        "\n",
        "# Print the metrics obtained from the predictions o/p\n",
        "print(outputs.metrics)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ebeb007-1f5a-4519-9358-97f78d144dba",
      "metadata": {
        "id": "3ebeb007-1f5a-4519-9358-97f78d144dba"
      },
      "outputs": [],
      "source": [
        "outputs = trainer.predict(test_data)\n",
        "\n",
        "# Extract the true lablels from the model o/p\n",
        "y_true = outputs.label_ids\n",
        "\n",
        "# Predict the labels\n",
        "y_pred = outputs.predictions.argmax(1)\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
        "\n",
        "# Define a func to plot a confusion metrics\n",
        "def plot_confusion_matrix(cm, classes,title='Confusion matrix', cmap=plt.cm.Blues, figsize=(10,8)):\n",
        "\n",
        "    # Create a figure with a specified size\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # Display confusion metrics\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    # Define tickmarks and labels for the classes\n",
        "    ticl_marks = np.arange(len(classes))\n",
        "    plt.xticks(ticl_marks, classes, rotation=90)\n",
        "    plt.yticks(ticl_marks, classes)\n",
        "\n",
        "    fmt = '.0f'\n",
        "\n",
        "    thresh = cm.max() / 2.0\n",
        "\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# calculate accuracy and F1 score\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "# Display\n",
        "print(f\"Accuracy: {accuracy:4f}\")\n",
        "print(f\"F1 Score: {f1:4f}\")\n",
        "\n",
        "# Get the confusion metrix if theres a small no. of labels\n",
        "if(len(labels_list) <= 150):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plot_confusion_matrix(cm, labels_list, figsize=(8, 6))\n",
        "\n",
        "# Finally display classification report\n",
        "print()\n",
        "print(\"Classification Report\")\n",
        "print()\n",
        "print(classification_report(y_true, y_pred, target_names=labels_list, digits=4))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0RaZuKvOS16k",
      "metadata": {
        "id": "0RaZuKvOS16k"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "trainer.save_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}